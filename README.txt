神经网络接收的都是数值型数据，所以训练网络第一步要做的就是将语言转换为数据值向量。其中最容易想到也是最简单的是one-hotreprestation，用一个很长的向量表示一个词，向量中只有一个1，其它都是0，如：
	” 差评“：[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]
	“坑”：[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
这种方式非常简洁，容易理解，但缺点也很明显，首先生成的矩阵过于稀疏，会产生维度灾难，更重要的是这种方式不能体现出词与词之间的关系，即使是相似的词在数据上也没有任何关系。基于上述情况，深度学习三巨头之一的Hinton于1986年提出DistributedRepresentation的方式，用一个n维向量表示，如[-0.192,0.293,0.123,…]，其中的值为(-1,1)之间的值。当然要把词表示成这样需求训练，这种表示的好处是向量的维度不高，一般在100到300之间，而且词与词之间的关联性可以由向量的距离表示，不在是孤立的。
训练词向量用的word2vec，它是Google于2013开源的词向量生成工具，如果想知道其中的原理可以在网上查找，而word2vec的实现方法也有很多，比如python, gensim，本文是用google的深度学习框架tensorflow实现，具体看程序word2vec.py，最终生成的词向量文件是word_vec.npy。




从第一节中的7个训练文件中各取部分数据，通过一句话的关键字，如：” 差评“、”坑“等将数据粗略分为两类，再通过人工进一步将数据细分，最终将数据集分为angry.txt和common.txt，程序为lable.py。由于网络中需要确定训练向量的长度，通过程序cal_len.py程序统计文本中单句的长度。
通过程序cal_model_data.py，最终得到训练样本的词向量modelMatrix.npy，这个文件将作为神经网络的输入。
	将第二节生成的词向量word_vec.npy和modelMatrix.npy作为神经网络输入，由于是个人训练用的自己的电脑，所以神经网络只有一个隐藏层，层中LSTM个数为35，程序是model_zg.py。最终生成文件夹result，其实就是tensorflow结构图和权重。


同样的方法将第一节中的测试文件向量化，分别取了新的分类好的5000生气数据和5000条正常交流的数据作为测试数据集，最终模型预测的正确率为：
生气数据正确率：0.9237
正常交流数据正确率：0.9178
